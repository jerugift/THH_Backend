SindhujaEmail: sindhuja121592@gmail.com Phone: (314)529-0116LinkedIn: www.linkedin.com/in/sindhuja-a-5a1597278PROFESSIONAL SUMMARY:* Data Engineering Expertise: Possesses over 9 years of experience in designing, developing, supporting, and maintaining big data applications on leading platforms like AWS, Azure, and Google Cloud Platform (GCP). Demonstrates proficiency in crafting scalable and fault-tolerant systems, particularly on GCP.* Industry-Specific Data Solutions: Spearheaded big data implementations and transformations across diverse sectors, including retail, fast-moving consumer goods (FMCG), banking, financial services, and agriculture.* Cloud and Linux Proficiency: Proficient in resolving critical and complex technical issues across various Linux platforms including Red Hat Enterprise Linux, Oracle Enterprise Linux, SUSE, CentOS, and Ubuntu. Experienced in provisioning and managing Azure IaaS & PaaS services to ensure smooth operations and deployment.* Cloud and Hadoop Ecosystem Proficiency: Extensive background in AWS, Microsoft Azure, and the entire Hadoop ecosystem, focusing on the creation of highly available, scalable, and resilient systems.* Advanced Scripting and Data Engineering: Expert in data analysis and scripting using Python, PySpark, and Spark APIs, with a strong ability to develop sophisticated data structures, design efficient data pipelines, and engineer comprehensive data solutions.* Software Development Lifecycle (SDLC): Deep experience with both Waterfall and Agile methodologies throughout the SDLC, encompassing requirements analysis, design, specification, and testing.* NoSQL and Real-Time Processing: Skilled in designing key structures for real-time data processing using NoSQL databases and proficient in deploying and managing infrastructure with tools like Ansible, Puppet, and Chef.* SQL Query Writing: Experienced in writing complex SQL queries for MySQL, Oracle, SQL Server, and HiveQL.CI* Continuous Integration and System Administration: Developed and supported Jenkins-based continuous integration frameworks and managed WebSphere Application and Portal Servers.* Spark Utilization: Practical experience in utilizing Spark Streaming, Spark SQL, and Spark Core, including using Scala for data frame creation.* Cloud Technologies and Automation: Utilized Terraform across various cloud services including AWS Analytics and Azure IoT, and managed multiple automation projects using Jenkins, Selenium, and Junit.* Client and Stakeholder Engagement: Collaborated extensively with clients, stakeholders, and businesses of all sizes to lead comprehensive data engineering projects, develop analytics solutions, and convert complex business needs into practical implementations.* Data Management and Analysis Tools: Proficient in managing and maintaining Snowflake databases, with extensive knowledge of Python libraries such as Psycopg, NumPy, Pandas, PySpark, Pytest, Pymongo, and PyExcel, enhancing data analysis and processing capabilities.Technical SkillsCategoryTechnologies/ToolsOperating SystemsRed Hat Enterprise Linux, Oracle Enterprise Linux, SUSE, CentOS, UbuntuProgramming LanguagesPython, Scala, Shell Scripting, HiveQL, RPGIV, RPGLE, RPGFreeDatabasesRDBMS, MySQL, Teradata, DB2, Oracle, SnowflakeNoSQL DatabasesHBase, Cassandra, MongoDBContainer and Cluster ManagersDocker, KubernetesBig Data EcosystemHDFS, MapReduce, Spark, Yarn, Hive, Pig, Sqoop, Flume, Kafka, Oozie, Zookeeper, ImpalaHadoop TechnologiesApache Hadoop 1.X, Apache Hadoop 2.X, Cloudera CDH4/CDH5, HortonworksETL ToolsInformatica PowerCenter, Informatica Data Quality (IDQ), Alteryx Designer, Alteryx ServerCluster ServicesSUSE SAP, RHELMachine Learning TechniquesRegression, Decision Trees, Clustering, Random Forest Classification, SVM, NLPBI ToolsTableau, Power BI, SQL Server Reporting Services (SSRS)Web DevelopmentHTML, XML, CSSWeb Scraping ToolsGo Anywhere ToolOperating SystemsWindows (XP/7/8/10), Linux (Ubuntu, Centos)Configuration ManagementPuppet, Ansible, ChefIDE ToolsEclipse, Jupyter, Anaconda, PyCharm, RDI (Rational Developer for i), VS CodeDevelopment MethodologiesAgile, Waterfall, Scrum, Kanban (using tools like JIRA)Cloud PlatformsAWS, Azure, Google Cloud PlatformSource ControlGit, GitHubUnit TestingJunit, Python libraries (Pytest, Unittest)Scripting LanguagesPowerShell, Python, Ruby, Bash, YAML, JSONSQL Query WritingComplex SQL queries for MySQL, Oracle, SQL Server, and HiveQLLyondellBasell Industries. Houston, TX Role: Senior Data Engineer March 2022   Present Responsibilities:* Spark and PySpark: Utilized for complicated facts processing pipelines, improving batch and circulation processing to enhance performance.* AWS Cloud Services: Managed scalable and secure cloud infrastructure using EC2, S3, and Redshift for data storage and processing.* Python and Pandas: Conducted data manipulation and complex transformations to support analytics and machine learning models.* HiveQL and Hive: Optimized data queries and managed large-scale data storage within HDFS.* MySQL and Oracle: Administered relational databases for robust data storage and retrieval mechanisms.* SOAP and REST APIs: Integrated external data sources to enhance system capabilities and data ingestion.* Snowflake: Deployed for cloud data warehousing, supporting real-time analytics and decision-making processes.* Data Processing and Cloud Services: Managed scalable and secure cloud infrastructure using AWS Cloud Services (EC2, S3, Redshift).* Automation and Diagnostics: Conducted data manipulation and complex transformations using Python and Pandas to support analytics and machine learning models.* NiFi: Automated data flows, improving data aggregation and distribution efficiency.* Cassandra: Managed high-availability data applications, ensuring minimal latency in data access.* Spark SQL: Executed SQL queries within Spark environments to streamline data analysis tasks.* Cloudera: Managed Hadoop ecosystems, optimizing big data processing and analytics.*  Apache Kafka: Built real-time records streaming talents to aid event-pushed architectures.* Sqoop: Facilitated efficient statistics transfers between Hadoop and relational databases.* Jenkins, Eclipse, and Git: Employed Jenkins for CI/CD processes, used Eclipse as the primary IDE, and managed code versions and collaboration using Git.Environment: Spark, AWS, Python, Pandas, HiveQL, MySQL, Soap, Snowflake, NiFi, Cassandra, Spark SQL, PySpark, Cloudera, HDFS, Hive, Apache Kafka, Sqoop, Scala, Shell scripting, Linux, MySQL Oracle Enterprise DB, Jenkins, Eclipse, Oracle, Git,ESCO Technologies, Saint Louis, MO Role: Senior Data Engineer July 2021   February 2022 Responsibilities:* Advanced Data Pipeline Management: Spearheaded the improvement and optimization of records pipelines the usage of Apache Airflow, Azure Data Factory, and Talend, automating ETL strategies and making sure efficient statistics management across operational systems and cloud systems like GCP and Azure.* Real-Time and Batch Data Processing: Implemented real-time data streams with GCP's Dataflow and Pub/Sub and managed batch data ingestion using Hive and Sqoop, enhancing data availability and analysis capabilities for supply chain management.* Cloud Data Solutions and Security: Engineered secure, scalable cloud storage solutions using GCP Cloud Storage and Azure Data Lake, applying best practices to comply with stringent data privacy standards and ensure system resilience.* Analytics, Machine Learning, and Reporting: Developed automated pipelines for machine learning and analytics in Azure Databricks, crafted detailed reports and dashboards using JIRA, and performed advanced data analysis with Python libraries (Pandas, NumPy).* Power BI Integration: Developed interactive dashboards and reports in Power BI, integrated various data sources including SQL databases and cloud services, conducted advanced data analysis using DAX, optimized report performance, created custom visuals, collaborated with business analysts, implemented data security measures, and set up automated data refreshes to ensure up-to-date reporting.* Agile Methodology and Project Management: Leveraged JIRA to implement Agile methodologies, enhancing project delivery through effective sprint planning, backlogs management, and team collaboration, leading to improved project transparency and milestone achievements.* Data Integrity and Performance Optimization: Created and optimized data validation programs using Scala, managed database operations and query optimization across various databases (Oracle, SQL Server, MySQL), and developed Python UDFs for reusable data operations in Hive.* Informatica Integration: Designed, developed, and optimized complex ETL processes using Informatica PowerCenter and Informatica Data Quality (IDQ) to ensure data accuracy, integrity, and reliability; managed end-to-end data integration projects, collaborated with stakeholders, and provided production support to maintain data pipeline stability.* Alteryx Development: Developed advanced workflows and predictive models in Alteryx Designer to automate data extraction, transformation, and analysis; optimized workflows for performance, utilized Alteryx Server for automation, and collaborated with stakeholders to deliver comprehensive data solutions supporting decision-making processes.* Continuous Integration and Deployment Infrastructure: Established a robust CI/CD pipeline using GitHub, Azure, and Docker, facilitating seamless code deployments and integration to support continuous development and testing environments.Environment: Apache Airflow, GCP (Dataflow, Pub/Sub, Cloud Storage), Azure (Data Factory, Data Lake, SQL, DW, Databricks), Informatica, Alteryx, GitHub, Docker, Talend Big Data Integration, Snowflake, Oracle, SQL Server, MySQL, MongoDB, HBase, Cassandra, Python (PySpark, Pytest, Pymongo, PyExcel, Psycopg), Scala, Hive, Sqoop, Matplotlib, NumPy, Pandas.Prime Communications, Sugar Land, Texas, HyderabadRole: Data Engineer December 2020   June 2021 Responsibilities:* Enhanced Data Strategy Development: Led the refinement of data extraction and integration strategies using Azure Data Factory, significantly enhancing data accessibility and integrity across diverse platforms, ensuring robust data pipeline management.* Cloud Infrastructure Automation: Engineered and fully automated cloud solutions leveraging AWS CloudFormation and Python scripting. This optimized the deployment and operational processes across both AWS and Azure environments, improving system efficiency and scalability.* Encapsulation and Abstraction: Skilled in encapsulating data and functions within classes to enhance modularity and ensure data hiding, while using abstraction to simplify complex systems by modeling classes that capture essential characteristics, thus facilitating higher-level thinking and design.* Big Data Systems Management: Configured and sustained Hadoop EMR clusters, utilizing Azure Data Lake for effective big data storage and processing. Integrated Kafka and Storm for real-time analytics, enhancing the timeliness and reliability of data-driven insights.* Power BI Development: Led the refinement of data extraction and integration strategies using Azure Data Factory, developed and implemented dynamic data visualizations with Power BI, integrated and transformed data from various sources, performed advanced data analysis, optimized report performance, collaborated with stakeholders, ensured data security and governance, and automated data refreshes and scheduling to maintain accurate and timely reporting.* Comprehensive Data Migration and Integration: Spearheaded extensive data migration projects utilizing PowerShell and SSIS to ensure seamless integration of legacy systems into updated, cloud-based architectures, maintaining data integrity throughout the transition.* Advanced Analytics Solutions: Developed and implemented cutting-edge analytics solutions using Azure Databricks. Designed and executed dynamic data visualizations with Tableau and Python, which facilitated strategic decision-making across the organization.* Operational Efficiency in Data Processing: Employed Sqoop for efficient data transfer between Hadoop EMR and SQL databases, optimizing data movement and integration workflows to support continuous data availability and system performance.* Strategic Data Handling and Visualization: Created sophisticated data models and visual analytics using Python and Tableau, which allowed for complex data interpretation and drove significant business decisions based on actionable insights generated from large datasets.* System Performance and Data Analysis Enhancement: Conducted thorough system performance evaluations and data quality checks using SSIS and Python, ensuring optimal system operation and precise data analysis to support enterprise goalsEnvironment: Azure Data Factory, AWS CloudFormation, Python, Hadoop EMR, Azure Data Lake, Kafka, Storm, Power BI, PowerShell, SSIS (SQL Server Integration Services), Azure Databricks, Tableau, Sqoop, SQL databases, Python, SSIS.AvMed, Miami, FLRole: Data Engineer March 2018   November 2020 Responsibilities:* Data Integration and Workflow Automation: Developed and managed SSIS packages to facilitate data extraction from SQL Server tables to Excel. Utilized Apache Nifi to automate data flows between Hadoop clusters and SQL Server, enhancing system interoperability and data synchronization.* Database Optimization and Scripting: Enhanced SQL Server efficiency through T-SQL optimizations and automated routine tasks within SSIS packages using C# scripting, improving database performance and task automation.* High-Performance Distributed Computing: Built distributed, high-performance data processing systems using Spark and Scala, specifically tailored for healthcare CRM systems to streamline data management and enhance customer interactions.* Advanced Data Storage Solutions: Engineered data storage solutions that involved storing large datasets in Parquet format on HDFS. Processed these datasets using Spark SQL and Azure Databricks, optimizing data handling and computation processes.* Complex Data Analytics and BI Development: Executed sophisticated data queries on Hadoop using Impala and managed large-scale data analysis of over 100TB daily using Redshift, developing BI solutions that ensured data integrity and operational efficiency.* Data Validation and Pipeline Management: Established robust data pipelines that supported data validation, cleansing, and loading processes into Azure SQL databases. Utilized Redshift for ongoing data quality assurance and integrity checks.* Data Visualization and Reporting: Leveraged MS Excel to create dynamic data visualizations and reports that supported data analysis findings and insights, helping stakeholders understand complex data trends and patterns easily.* NoSQL Database Management: Managed and optimized data transactions and storage solutions using MongoDB, a NoSQL database, to handle unstructured data efficiently, facilitating faster access and scalability.Environment: SSIS, SQL, T-SQL, Scala, Spark, Spark SQL, Apache Nifi, Hadoop, HDFS, MongoDB, Azure Databricks, Impala, Redshift, MS Excel.Graindor, HyderabadRole: Junior Data Engineer November 2013 - October2017 Responsibilities:*  Advanced Data Analysis and Visualization Techniques: Utilized R and MS Excel to behavior complete records analysis and create visualizations that become aware of key client and operational developments. This worried the use of advanced statistical methods and graphical representations to assist commercial enterprise decision-making.* Development of Machine Learning Models: Developed and refined predictive models the use of Python and Spark, leveraging libraries such as NumPy, SciPy, Pandas, scikit-examine, and Seaborn. These models had been especially designed to investigate and predict consumer behavior and operational efficiencies, improving strategic effects.* Scalable Cloud Infrastructure Deployment: Engineered strong, scalable cloud answers using AWS technologies which include Hadoop frameworks like Cloudera and Hortonworks. Managed complex records flows and massive-scale data storage answers, optimizing infrastructure for top overall performance.* Automation of Data Pipelines: Automated ETL techniques the use of AWS Kinesis and Glue to streamline statistics ingestion and integration. This enhancement notably improved records availability and operational performance, lowering guide exertions and errors charges.* Statistical Analysis and Business Intelligence Reporting: Conducted in-depth statistical analyses and crafted reviews the usage of Tableau. These sports were instrumental in imparting actionable insights that drove strategic business choices.* Version Control and Project Management: Managed supply code and assignment improvement the use of version manipulate systems such as GitHub, SVN, and CVS. This ensured high standards of code integrity, facilitated seamless team collaboration, and maintained challenge continuity across a couple of improvement tiers.* Real-Time Data Processing and Analysis: Utilized Kafka and Spark s PySpark and MLlib for actual-time data processing and analytics. Implemented streaming information solutions that allowed for the instant evaluation and response to incoming records streams, enhancing the responsiveness of statistics systems.Environment: R, MS Excel, Python, NumPy, SciPy, Pandas, scikit-learn, Seaborn, Spark, PySpark, MLlib, AWS, Hadoop, MongoDB, Cloudera, Hortonworks, Teradata, Kafka, PyCharm, GitHub, SVN, CVS-----END OF RESUME-----

Name: Sindhuja A
St. Louis, MO, United States
Phone: 3145290116
Email: xeu-u1z-9jy@mail.dice.com

Education

Experience

Years Of Experience: 2

Current Work Company: Lyondellbasell Industries
Current Work Position: Senior Data Engineer
Current Work Start Year: 2022

Company: Lyondellbasell Industries
Position: Senior Data Engineer
Start Year: 2022
Location:

Authorization: Green Card Holder
Desired Position: Senior Data Engineer

Work Permit Locations: US

Skills:
apache hive: 10 years, last used in 2024
apache sqoop: 10 years, last used in 2024
data analysis: 10 years, last used in 2024
python: 10 years, last used in 2024
algorithms: 9 years, last used in 2024
amazon redshift: 9 years, last used in 2024
cluster: 9 years, last used in 2024
data storage: 9 years, last used in 2024
database: 9 years, last used in 2024
hdfs: 9 years, last used in 2024
mapreduce: 9 years, last used in 2024
amazon s3: 7 years, last used in 2024
amazon web services: 7 years, last used in 2024
apache kafka: 7 years, last used in 2024
collaboration: 7 years, last used in 2024
iaas: 7 years, last used in 2024
machine learning: 7 years, last used in 2024
snow flake schema: 7 years, last used in 2024
statistics: 7 years, last used in 2024
streaming: 7 years, last used in 2024
data flow: 6 years, last used in 2024
data engineering: 10 years, last used in 2022
automation: 8 years, last used in 2022
business intelligence: 8 years, last used in 2022
data processing: 8 years, last used in 2022
reporting: 8 years, last used in 2022
analytics: 7 years, last used in 2022
cloud: 7 years, last used in 2022
numpy: 7 years, last used in 2022
real-time: 7 years, last used in 2022
etl: 5 years, last used in 2022
apache hadoop: 9 years, last used in 2021
operational efficiency: 7 years, last used in 2021
apache spark: 10 years, last used in 2020
operations: 9 years, last used in 2020
microsoft excel: 7 years, last used in 2020
qa: 7 years, last used in 2020

Downloaded Resume from Dice
7fe75c05e85eab1c05a9c8bd6e28b391e846749a


