 Sandeep G          sgollabi@gmail.com Consultant                 510-936-2511 Summary:   Extensive IT experience encompassing system design, administration, development, testing support, and production support in Data Warehousing, specializing in ETL, Reporting, and Data Modeling. Proficient in all phases of the Data Warehouse life cycle, with expertise in Informatica technologies and administration.   Proficient in leveraging Databricks platform for big data analytics and machine learning tasks, including data ingestion, transformation, and model development.   Experienced in designing and implementing end-to-end data pipelines on Databricks, integrating various data sources and formats to support analytics and reporting requirements.   Skilled in developing Apache Spark applications using Scala, Python, or SQL within the Databricks environment, optimizing code for performance and scalability with AWS.   Skilled in optimizing Databricks deployments on AWS for cost efficiency, leveraging features like spot instances, auto-scaling, and resource tagging to minimize.   Proficient in deploying and managing Databricks workloads on Amazon Web Services (AWS), leveraging services like Amazon EMR, Amazon S3, and AWS Glue for seamless data integration and processing.   Experienced in designing and implementing data lake architectures on AWS using Databricks, storing and processing large volumes of structured and unstructured data in Amazon S3 for analytics and machine learning.   Experienced in collaborating with data engineers, data scientists, and business stakeholders to understand requirements, iterate on solution designs, and deliver robust PySpark-based data solutions aligned with business objectives.   Led the design and implementation of Data Warehousing solutions following Ralph Kimball models with Star/ Snowflake Schema Designs, covering analysis, definition, database design, testing, and implementation.   Spearheaded ETL Architecture using Informatica PowerCenter (8.6.1/9.0.1/10.4) and Informatica Data Quality (8.6/9.0.1/10.4), ensuring seamless data integration and quality processes.   Managed the full development life cycle of Data Warehouse projects, including requirement analysis, design, coding, testing, and deployment.   Strong emphasis on documenting Databricks solutions, best practices, and architectural decisions, facilitating knowledge sharing and ensuring maintainability of developed solutions.   Proficient in reading from diverse data sources such as Mainframe datasets, COBOL, Oracle, SQL Server, Access DB, Flat files, Teradata, Siebel, and SFDC using Informatica Power Exchange.   Experienced in performing data manipulation and transformation tasks using Pyspark Data Frame operations, including filtering, aggregating, joining, and sorting data to prepare it for downstream analysis.   Integrated data from multiple sources including SFDC, SQL Server, Oracle, Flat Files, XML, and XSDs, ensuring comprehensive data consolidation.   Proficient in database partitioning, index creation, and performance tuning to optimize data retrieval and processing.   Designed and implemented database-level objects like views, materialized views, triggers, cursors, procedures, and functions using PL/SQL.   Developed and maintained Data Warehouse Administration Console (DAC) with different subject areas, facilitating streamlined execution and scheduling. Education: - Texas A&M Commerce Master's in Computer Engineering 2010, JNTU Bachelor's in Computer Sciences and Information Technology 2008.   Professional Experience: Languages     SQL, PL/SQL, SQL*Plus, UNIX Shell Scripting, Perl, C, C++, COBOLData Bases ORACLE 10g/9i/8i/, AS/400, MS SQL Server 2000/2005/DTS Packages, Teradata, Sybase, DB2 UDB, TOAD, SAP ETL Databricks, Informatica Power Center ( 9.0.1/10.2/10.4/10.5), Informatica Power Exchange.BI Tools OLAP) Business Objects, Crystal reports.Data Modeling Erwin3.5.5/3.5.2/4.0, Visio 2003Operating Systems Sun-Solaris, HP-Unix, NT, MS-DOS 6.1, Mac OSX 10.5/10.6/10.7, WIN-95/98/XP/VISTA/7.Office Tools OFFICE, Excel, MS Project, Visio, Power point, Word Perfect, and Corel draw.Networks                                Windows NT, Windows 2000, NovellOther Skills and ToolsMQ Series, TOAD, PL/SQL Developer, SQL Loader, Autosys and Web tools:  HTML, PHP, DHTML, XML, XSD           | Page 1 3M                                   Jan 21 - Present Sr. Informatica Databricks Developer This project is to facilitate environment of databricks and Informatica powercenter, I mainly worked on the migrating the informatica PC mappings to databricks code. And load the data into the AWS s3 bucket.     Working as Databricks, Informatica developer with PowerCenter and IDQ.    Experience in connecting Powerexchange AWS s3 and Redshift.   Migrating ETL mapping from informatica and pipelines in Databricks.   Worked on RDD and DataFrame techniques in PySpark for processing data at a faster rate.    Developed the batch scripts to fetch the data from AWS S3 storage and do require transformations.    Using Informatica connected with AWS Redshift and created a data mart in cloud database. And created a datalake and access from AWS Athena.   Experience in working on AWS Glue catalog for external schemas to create and access.    Extensively worked on Mapping Variables, Mapping Parameters, Workflow Variables and Session Parameters.   Used Workflow Manager for Creating, Testing and running the sequential and concurrent Sessions and scheduling them to run at specified time.    Used pre-session and post-session scripts for dropping and recreating indexes before and after loading data into target table to optimize performance.   Developed PL/SQL aggregation programs, stored procedures, and packages to solve some issues in which case performance enhanced a lot.   Extensively tuned the SQL queries which are being used as a part of different transformations such as look ups and source qualifiers.   Extensively optimized all the Informatica sources, targets, mappings and sessions by finding the bottlenecks in different areas and debugged some existing mappings using the Debugger to test and fix the mappings.   Migrated mappings, sessions, and workflows from Development to Testing and then to Production environments. Poly,                                                                                                Aug 18- Jan 21 Sr. Databricks Architect This project is to facilitate any environment or platform related & data related requirements and challenges in Merchandise space. Integrate test data provisioning & encryption/masking solution with CICD build pipeline in Jenkins. Provide solution to any existing data issue & database performance related issues for applications to run smooth with the underlying data.    Used Databricks, informatica PowerCenter for developing the mappings.   Enable pipelines from informatica PC to Databricks notebooks with Pyspark.   Used AWS cloud services for archive the large data set into s3.   Using Databricks connected with AWS Redshift and created a data mart in cloud database. And created a data lake and access from AWS Athena.   Experienced in designing and implementing ETL (Extract, Transform, Load) pipelines using PySpark, handling large volumes of structured and semi-structured data from diverse sources for data warehousing and analytics.   Experience in working on AWS Glue catalog for external schemas to create and access.   Migrating ETL mapping from informatica and pipelines in Databricks and IICS.   Experience in Databricks workspace, Catalog, workflow, and compute for the Jobs.   Worked on Databricks SQL editor, queries, dashboards for the data set.   Collaborating with stakeholders to gather requirements and define specifications for ETL mappings and workflows to ensure accurate and efficient movement of sentinel logs from Oracle Server to SQL Server ETL Database.   Developing and maintaining PowerShell scripts to automate the scanning of network directories for file tracking activity logs and loading them into SQL Server tables, ensuring data integrity and timely processing.   Writing and optimizing functions and stored procedures to extract relevant process logs from sentinel logs for SLA monitoring and reporting purposes, ensuring accuracy and efficiency in data retrieval.   Configuring and managing automation of ETL processes, file tracking, and SLA monitoring tasks using Tidal Scheduler, ensuring seamless execution and scheduling of critical data processes.   Creating comprehensive documentation for the Run team, including operational procedures, troubleshooting guides, and process flow diagrams, to facilitate smooth operation and maintenance of ETL workflows and file tracking activities.   Developing detailed mapping design documents outlining the data flow, transformations, and dependencies within ETL mappings and workflows, ensuring clarity and consistency for development and maintenance purposes Brocade San Jose, CA                                                                                           Oct 15 - Aug 18            | Page 2 ETL Developer/Support (L1 & L2) Brocade provides innovative network solutions that help the world s leading organizations transition smoothly to a virtualized world where applications and information can reside anywhere. The prime responsibility of our team is to build customer database hub (CDH) means party-customer account information using web services such as SOAP. We are loading data from different sources to staging area by cleansing data to do this we are using Address Doctor services and Duns Numbers from Duns and Bradstreet (DNB). After getting data as per business requirement from DNB and Address Doctor we are loading data to the warehouse. Responsibilities:   EBS (Enterprise Business Solutions) is integrated with Oracle R2, POS (point of sales) is integrated with Infonow Source systems and SFDC (Salesforce.com) is integrated with  SFDC source system, We extract the data from this systems into data warehouse (Customer Data Mart) using power exchange , Flat files, and relational database tables.   Closely work with BSA team and I was involved in project ETL Development/Architect.   Ultimate goal is in the project to identify the GOLDEN RECORD for the customers.      We have developed the CUSTOME MDM process using Informatica PowerCenter, Data Quality and Oracle applications with TCA model   Extensively worked on WEB Services Consumer transformation, SQL transformation and Normalize transformation.   Using IDQ we have developed the validation rules for example Address Profile /Standardizing / Clean / Swap etc.   Using Unix Scripts extracted the data from FTP to location informatica server, and also, we have created the process to Archive the file in UNIX box.   Closely worked with DNB (Dun and Bradsheet) team to get the REAL TIME / OPTIMIZE files response with fixed format.   Worked on informatica WEB Services Consumer transformation, to send request from data base to SOA Xml to get the response from DNB / Address doctor. This response files we load into DNB STG tables are Address Doctor STG tables.    Deploy the database changes from Development instance to all environments through Shell wrapper scripts.   Supported QA and UAT team to validate the incremental load and fixed bugs raised by QA and UAT   Configured DAC  Data warehouse Access control for running of ETL jobs. Worked to deploy and optimize performance of BPM applications   Worked on control table procedures to run the incremental loading, truncate packages, bulk load insert and update procedure etc.   Used deployment groups to migrate the informatica mappings from Development instance to all reaming instances.    Extensively worked on Informatica/SQL performance Tuning using partitions.   Wrote the SQL scripts to fix the history data set.   In the SFDC extensively worked on Assets, Agreements, Contract line item, Agreement Contract, Partner_Classification__c, and Account data extracts for sales/Service info.   Extensively worked on PL/SQL and Worked with offshore India team. Autodesk             San Rafael, CA                                                                                        Nov 13 - Oct 15 Informatica ETL Admin/Dev Scope: UCM and LEM projects represented a significant opportunity for Autodesk to Autodesk has come a long way since our founding more than 25 years ago as a pioneer in the world of computer-aided design (CAD). Our flagship product, AutoCAD software, has become synonymous with CAD and is a fixture in design shops worldwide. We have industry-leading 3D solutions for industrial design and manufacturing; architecture, engineering and construction; as well as media and entertainment. Today we stand as a world leader in 3D design, engineering, and entertainment software.   Setting up and configuring enterprise Informatica Implementation with PowerCenter 8.X (8.6 and further preferable) on UNIX and Oracle 10g and greater    Extensively work in installing/configuring PowerExchange    Installation and configuration on a multi-server environment and securing Cognos10 BI.   Administering, deploying and maintaining Models, reports, and cubes to all environments.   Set up required permissions in the Cognos Portals for the User group for security purposes.   Create and maintain the software migration process, coordinator and implement improvements to the migration process.    Closely work with Informatica Corp for product issues, licensing and understanding product roadmaps    Maintaining security through effective management of users/groups   Working with off sure (India) team.    Extensively worked on PL/SQL.    Extensively worked in Implementation of patches and upgrades. T-Mobile                        | Page 3 Atlanta, GA                                                                                        Nov 10   Oct 13 Java Developer and Support   Knowledge of Object-Oriented Programming and design   Work experience with at least one of the Java frameworks   Knowledge of relational databases, SQL and ORM   Knowledge of web technologies like HTML, CSS, Javascript, and JQuery   Proficient in Java, JavaScript, Java Applets, Java Servlets, Java Beans   Excellent communication skills and interpersonal skills   Motivated team player and ability to reflect leadership skills            | Page 4

----- START OF EMAIL CORRESPONDENCE -----

Return-Path: <sgollabi@gmail.com>Received: from mail.notificationforyou.com ([10.10.21.41])by mail.jobdivaemail.com (Kerio Connect 9.2.1) with ESMTPSfor pozent@jobdivaemail.com;From: sgollabi@gmail.comTo: pozent@jobdivaemail.comMessage-ID: <1248805888.141285.1712335110227@localhost>Subject: Application for 23-01863Content-Type: multipart/mixed;Date: Fri, 5 Apr 2024 12:37:05 -0400Resume